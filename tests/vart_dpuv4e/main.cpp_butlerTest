// Copyright 2021 Xilinx Inc.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//      http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

#include <assert.h>
#include <stdio.h>
#include <stdlib.h>
#include <sys/stat.h>

#include <cassert>
#include <cmath>
#include <cstdio>
#include <fstream>
#include <iomanip>
#include <iostream>
#include <queue>
#include <string>
#include <vector>
#include <chrono>
#include <thread>
#include <vart/runner.hpp>
//#include <vart/dpu/dpu_runner_ext.hpp>
#include <dpu_runner_ext.hpp>
#include <xir/graph/graph.hpp>
#include <xir/tensor/tensor.hpp>
#include "common.h"
/* header file OpenCV for image processing */
using namespace std;
static long getFileSize(std::string filename)
{
    struct stat stat_buf;
    int rc = stat(filename.c_str(), &stat_buf);
    return rc == 0 ? stat_buf.st_size : -1;
}

/**
 * @brief Entry for runing ResNet50 neural network
 *
 * @note Runner APIs prefixed with "dpu" are used to easily program &
 *       deploy ResNet50 on DPU platform.
 *
 */
int main(int argc, char* argv[]) {
  // Check args
  if (argc != 4) {
    cout << "Usage of resnet50 demo: ./resnet50 [xmodel_file] [meta.json] [imageDir]" << endl;
    return -1;
  }

  string xmodel_filename = argv[1];
  int numImgs = 16;
  int batchSz = 4;
  string meta = argv[2];
  //bool goldenAvailable = 0;
  //bool verbose = 1;
  string img_dir = argv[3];
  assert((numImgs%batchSz)==0);
  const unsigned num_queries_ = numImgs/batchSz;

  //std::unique_ptr<xir::Graph> graph0 = xir::Graph::deserialize(xmodel_filename);
  //auto subgraph0 = graph0->get_root_subgraph();
  //std::map<std::string, std::string> runset;
  //runset.emplace("run","librt-engine.so");
  ////subgraph0->children_topological_sort()[1]->set_attr<std::string>("kernel", "DPUCVDX8H");
  //subgraph0->children_topological_sort()[1]->set_attr("runner", runset);
  ////subgraph0->children_topological_sort()[1]->set_attr<std::string>("xclbin", "/group/dphi_cloud/v4e_rel/20200923/v4e_8pe_300MHz_xilinx_vck5000-es1_gen3x16_1_202010_1_a29ff10_Sep16Wed1614.xclbin");
  //graph0->serialize("./dpu.xmodel");


  std::unique_ptr<xir::Graph> graph = xir::Graph::deserialize("./dpu.xmodel");
  auto subgraph = get_dpu_subgraph(graph.get());
  //std::vector<xir::Subgraph *> subgraphs = graph->get_root_subgraph()->children_topological_sort();
  std::unique_ptr<xir::Graph> graph2 = xir::Graph::deserialize("./dpu2.xmodel");
  auto subgraph2 = get_dpu_subgraph(graph2.get());
  vector<xir::Subgraph *> sub;
  sub.emplace_back(subgraph[0]);
  sub.emplace_back(subgraph2[0]);
  thread workers[stoi(argv[2])];
  for (unsigned k=0; k < stoi(argv[2]); k++)
  {
    workers[k] = thread([&,k]() {

    //auto r2 = vart::Runner::create_runner(subgraph2[0], "run");
    auto r = vart::Runner::create_runner(sub[k%2], "run");
    auto runner = r.get();
    //auto runner2 = r2.get();
    auto inputs = dynamic_cast<vart::dpu::DpuRunnerExt*>(runner)->get_inputs();
    auto outputs = dynamic_cast<vart::dpu::DpuRunnerExt*>(runner)->get_outputs();
    auto output_tensors = runner->get_output_tensors();
    //std::unique_ptr<cpuUtil> cpuUtilobj_;
    //cpuUtilobj_.reset(new cpuUtil(meta, goldenAvailable, verbose, img_dir, num_queries_));
      if (k%2 ==0) {
        std::vector<vart::TensorBuffer*> inTensors;
        void *codePtr = NULL;
        std::string inputbin = "./tests/app/models/v4e_resnet50/input.bin";
        unsigned int size = getFileSize(inputbin);
        if (posix_memalign(&codePtr, getpagesize(), size))
          throw std::bad_alloc();
        auto infile = ifstream(inputbin, ios::in | ios::binary);
        for (unsigned i=0; infile.read(&((char*)codePtr)[i], sizeof(int8_t)); i++);
        std::vector<std::unique_ptr<vart::TensorBuffer> > tbufs;
        for (int bi=0; bi < 8; bi++)
        {
          memcpy((void*)inputs[bi]->data().first, codePtr,size);
        }
     }
     auto t1 = std::chrono::high_resolution_clock::now();
     //std::cout<<"Loading "<<num_queries_*4<<" Images ..."<<std::endl;
          //cpuUtilobj_->fillInData(i, inputs);
    //for (unsigned i=0; i < num_queries_; i++)
    for (unsigned i=0; i < 10; i++)
    {
      auto ret = (runner)->execute_async(inputs, outputs);
      (runner)->wait(uint32_t(ret.first), -1);
          //cpuUtilobj_->postProcess(outputs, i);
          //
      if (k%2 ==0) {
        const auto mode = std::ios_base::out | std::ios_base::binary | std::ios_base::trunc;
        //for (int bi=0; bi < 8; bi++) {
        int bi=0;
          for (int t=0;t<3;t++) {
            auto output_file = "./output" +  to_string(i)+to_string(k) +to_string(t)+ to_string( bi) + ".bin";

            std::ofstream(output_file, mode).write((char*)outputs[bi*3+t]->data().first, output_tensors[t]->get_element_num());
          }
        //}
      }
    }


   });
  }
  for (auto &w : workers) {
    if (w.joinable()) w.join();

  }


  auto t2 = std::chrono::high_resolution_clock::now();
//  std::chrono::duration<double> elapsed = t2-t1;
//  std::cout << "Elapsed: " << elapsed.count() << std::endl;
//  std::cout << "QPS: " << num_queries_/elapsed.count() << std::endl;
  return 0;
}
